---
layout: post
title:  " Dropout과 앙상블 "
categories: MachineLearning
author: goodGid
---
* content
{:toc}


<iframe width="560" height="315" src="https://www.youtube.com/embed/wTxMsp22llc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>


---


## Solution for overfitting


![](/assets/img/machine_learning/ML_10_3_1.png)

 


1. 학습 데이터를 늘리자.

2. features를 줄이면 되는데 굳이 할 필욘 없다.

3. `Regularization` : Let's not have too big numbers in the weight <br> 

Q. “Let's not have too big numbers in the weight”가 무슨 말이지 ?
{: .notice}

```
Weight값으로 너무 큰 값을 주지말자는 뜻이다.
학습 데이터가 너무 넓은 범위에 분포되어 있으면
W값이 모든 경우를 처리하기 위해서
값이 커질수가 있다. 그런 경우를 방지하기 위해
W값은 줄이되 모든 경우를 포함할 수 있도록 
Regularization하자는 것이다.
( 약간 soft_max처럼 [ 0 ~ 1 ]사이로 줄이는 느낌이랄까 ? )
( 지극히 주관적인 생각이라 틀린 답변일수있다.)
```

<br>


![](/assets/img/machine_learning/ML_10_3_2.png)

 

* 각각 W(Element)를 제곱한 값을 더한게 최소화가 되게 하자.


---


## Dropout


![](/assets/img/machine_learning/ML_10_3_3.png)

 

Neural Networks에는 

`Solution for overfitting`에 대한 해결책이 

1가지 더 있다. 

바로 `Dropout`이다.

<br>

Random하게 어떤 neurons을 squeeze하자.


![](/assets/img/machine_learning/ML_10_3_4.png)

 

<br>

그런데 Random하게 설정을 해도 될까? ...

가.능.하.다.

<center><b> How ? </b></center>

<br>


![](/assets/img/machine_learning/ML_10_3_5.png)

 

각각의 neurons들은 

각각의 정보를 갖고 있다.

<br>

그래서

일부는 쉬게하고 

남은 것들로 훈련을 시킨다.

<br>

그리고 

마지막에 쉬게한 것들을 총 동원해서

예측을 한다.


---

## Dropout Implementation code


![](/assets/img/machine_learning/ML_10_3_6.png)

 


주의할 점은 

학습 할 때만 dropout을 한다.

실전에서는 dropout을 하지 않는다.


``` python

# Train:
sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})

# Evaluation:
print('Accuracy:', sess.run(accuracy, feed_dict={
      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))

```



---

## What is Ensemble ?



![](/assets/img/machine_learning/ML_10_3_7.png)

 


Ensemble 앙상블이라 읽는다.

100개의 Data가 있을 때

1명한테 100번 학습을 시키는게 아니라

10명한테 10번 씩 학습을 시키는 것이다.

<br>

독립적으로 모델을 학습시키고

나중에 합치는 것이다.

실제로 정확도도 더 높게 나온다.

