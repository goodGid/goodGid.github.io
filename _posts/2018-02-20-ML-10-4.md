---
layout: post
title:  " NN, ReLu, Xavier, Dropout, and Adam "
categories: MachineLearning
author: goodGid
---
* content
{:toc}


<iframe width="560" height="315" src="https://www.youtube.com/embed/6CCXyfvubvY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>


---

이번 강의는 

Neural Net을 사용하면서 

굉장히 유용한 Tip들에 대해 알아보자.

실습 관련된 코드

[전체 코드](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-10-2-mnist_nn.py)

---

## Softmax classifier for MNIST


![](/assets/img/machine_learning/ML_10_4_1.png)

 

---


## NN for MNIST



![](/assets/img/machine_learning/ML_10_4_2.png)

 


그런데 여기서 의문점이 생겼다.

<br>

Q. 레이어 사이에 중간 연결 갯수가 어떤 영향을 끼칠까? 
{: .notice}

### 1. 적음

``` python

# weights & bias for nn layers
W1 = tf.Variable(tf.random_normal([784, 1]))
b1 = tf.Variable(tf.random_normal([1]))
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

W2 = tf.Variable(tf.random_normal([1, 256]))
b2 = tf.Variable(tf.random_normal([256]))
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

W3 = tf.Variable(tf.random_normal([256, 10]))
b3 = tf.Variable(tf.random_normal([10]))
hypothesis = tf.matmul(L2, W3) + b3


---

Accuracy: 0.2248
Label:  [1]
Prediction:  [3]

```


### 2. 보통


``` python

# weights & bias for nn layers
W1 = tf.Variable(tf.random_normal([784, 256]))
b1 = tf.Variable(tf.random_normal([256]))
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

W2 = tf.Variable(tf.random_normal([256, 256]))
b2 = tf.Variable(tf.random_normal([256]))
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

W3 = tf.Variable(tf.random_normal([256, 10]))
b3 = tf.Variable(tf.random_normal([10]))
hypothesis = tf.matmul(L2, W3) + b3


---
Accuracy: 0.9438
Label:  [0]
Prediction:  [0]


```



### 3. 많음

``` python

# weights & bias for nn layers
W1 = tf.Variable(tf.random_normal([784, 1000]))
b1 = tf.Variable(tf.random_normal([1000]))
L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

W2 = tf.Variable(tf.random_normal([1000, 256]))
b2 = tf.Variable(tf.random_normal([256]))
L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

W3 = tf.Variable(tf.random_normal([256, 10]))
b3 = tf.Variable(tf.random_normal([10]))
hypothesis = tf.matmul(L2, W3) + b3


---

Accuracy: 0.9635
Label:  [2]
Prediction:  [2]

```


3가지 예에서 볼 수 있듯이

중간 연결 갯수가 많을수록 

정확도가 높아진다.


---


## Xavier for MNIST


![](/assets/img/machine_learning/ML_10_4_3.png)

 



initialize를 잘해주기 위해

`Xavier Algorithm`을 사용하자.


``` python

W1 = tf.get_variable("W1", shape=[784, 256],
                     initializer=tf.contrib.layers.xavier_initializer())

```

기존 코드와 크게 달라진 점은 없다.

단지 초기화해주는 부분만 바뀌었다.


<br>

그 결과는 

Epoch이 1일 때부터

Cost가 상당히 낮다.

즉 초기값이 

굉장히 아주 잘 스마트하게

`initialize` 되었다고 볼 수 있다.


---


## Deep for MNIST



![](/assets/img/machine_learning/ML_10_4_4.png)

 


깊고 넓게 진행해본다.

그런데 

`Xavier for MNIST` 결과 정확도보다 떨어진다.

데이터에 따라 요인에 의해 떨어질수도 있지만,

여기서는 아마도 `overfitting` 때문이다.

<br>

이 `overfitting`을 막기 위한 

방법 중 하나가

`dropout`이다.

---

## Dropout for MNIST



![](/assets/img/machine_learning/ML_10_4_5.png)

 

dropout이란

Network이 overfitting이 되지 않도록

학습시킬 때

일부만 선별적으로 학습을 시키는 것이다.

<br>

그렇다면 몇 %를 선별할 것인가 ?

통상적으로 train을 할 때는 0.5 ~ 0.7로 한다

<br>

주의할 점은

실전에서는 반드시 1로 해야한다.




``` python

# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing
keep_prob = tf.placeholder(tf.float32)

L2 = tf.nn.dropout(L2, keep_prob=keep_prob)

```



---


## Optimizers


![](/assets/img/machine_learning/ML_10_4_6.png)

 

우리는 주로 tf.train.GradientDescentOptimizer를 사용하였다.

여러가지 종류가 있으니

어떤 것이 가장 좋은지는 

알아서 적절하게 선택해서 사용하자.

앞으로는 더 효율적인

`tf.train.AdamOptimizer`을 사용하자.


---

## Summary


![](/assets/img/machine_learning/ML_10_4_7.png)

 